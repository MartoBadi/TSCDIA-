# Meeting Transcription

  Meeting started: 1/4/2025, 07:51:25
  Duration: 96 minutes
  Participants: Andrea Alvarez, Antonella Diaz Caro, Dana Reyes, Deisy Rios, Diego Antonic, Diego Ferrari, Emilce Alconz, Florencia Ortiz Candeias, Hernán Hamra, Lautaro Iriart, Lorena Sierra, Mariana Morales, Martín Badino, Matias Arias, Miguel Carlos Pita, Miriam Coman, Nazarely Gomez Abularach, Nuria Arnaud, Rafael Rubiano Ramirez, Ricardo Nuñez, Solange Demey, Solange K. Wladimirsky, Victoria Spandonari

  [View original transcript](https://app.tactiq.io/api/2/u/m/r/xpnNeHgMaCwoV8AW99fE?o=txt)

  

  
  
  

  ## Transcript

  00:00 Miguel Carlos Pita: Buenas tardes, hola.
00:07 Hernán Hamra: Hola, qué tal? Buenas tardes.
00:08 Solange Demey: Buenas tardes.
00:08 Miguel Carlos Pita: Hola, cómo están?
00:13 Martín Badino: Bien.
00:17 Miguel Carlos Pita: Me alegro. Éramos cinco minutitos, no más para comenzar. Sí, no sé si se conectan. Los que faltan y 8 y 5 empezamos.
05:17 Miguel Carlos Pita: Bueno, cómo están?
05:20 Nuria Arnaud: Buenas profe.
05:21 Rafael Rubiano Ramirez: Hola Miguel buenas noches.
05:22 Miguel Carlos Pita: Hola, buenas noches, cómo andan? Espero que bien.
05:28 Solange K. Wladimirsky: Bien, gracias.
05:31 Miguel Carlos Pita: Bueno, bueno, paso lista y comenzamos la clase.
05:34 Speaker: Hola, estoy transcribiendo esta llamada con mi extensión Tactiq AI: https://tactiq.io/r/transcribing
05:43 Miguel Carlos Pita: halcón
05:45 Emilce Alconz: presente
05:50 Miguel Carlos Pita: Álvarez Anthony
05:57 Diego Antonic: Buenas noches, profe presente.
05:59 Miguel Carlos Pita: Qué tal llegó? Arias
06:05 Matias Arias: presente
06:10 Miguel Carlos Pita: Arnaud
06:12 Nuria Arnaud: presente
06:16 Miguel Carlos Pita: vadino Martín Barros Erika
06:36 Miriam Coman: presente
06:43 Miguel Carlos Pita: Qué tal? días Ferrari Diego
06:58 Antonella Diaz Caro: Presente!
06:59 Solange K. Wladimirsky: está presente Antonella
07:03 Miguel Carlos Pita: Hacia ahí escribió se estaba con la vista.
07:06 Antonella Diaz Caro: No me funciona el microfono :(
07:07 Diego Ferrari: hola, buenas noches acá Diego
07:07 Miguel Carlos Pita: la lista perfecto días claro presente Diego presente Gómez abollarache
07:15 Nazarely Gomez Abularach: Presente profe, buenas noches.
07:17 Miguel Carlos Pita: Qué tal? Cómo estás? Buenas noches. Gorosito Sandra Hernán hasta presente yo
07:31 Hernán Hamra: Presente buenas noches.
07:33 Miguel Carlos Pita: Hola. Y algo.
07:37 Lautaro Iriart: presente
07:39 Miguel Carlos Pita: la verdad la abuela y Facundo morales a pasa
08:01 Mariana Morales: presenta
08:08 Miguel Carlos Pita: no Ortiz candelas
08:22 Florencia Ortiz Candeias: presente
08:27 Miguel Carlos Pita: Estefanía
08:30 Dana Reyes: presente profe
08:31 Miguel Carlos Pita: Hola. ríos Daisy Rubiano creo que está presente. Hola, qué tal? sierra Marín
08:56 Lorena Sierra: presente
09:03 Victoria Spandonari: Hola, buenas noches, presente.
09:05 Miguel Carlos Pita: Buenas noches. Hola, buenas, repito los que no anoté.
09:08 Solange K. Wladimirsky: Presente profe, buenas noches.
09:19 Miguel Carlos Pita: listo para ahí está, ya te pongo presente después Álvarez Álvarez Andrea y michilini ríos Daisy bueno Bueno, espero que anden todos bien. arrancamos clases de minería de datos la única clase que va a haber esta semana ya ayer Lo avisé de todas maneras, pero sé que no todos no todos ustedes están. En la materia de los días lunes, mañana es el el feriado por el por Malvinas así que no vamos a tener clases. Esta sería la única clase de la semana en principio.
10:35 Miguel Carlos Pita: También algo que la clase pasada la clase del lunes mejor dicho les comenté que la semana pasada tuve un inconveniente en realidad, un imponderable no no pude. Modificar una cuestión laboral personal mía y se me se me complicó poder dar clases, así que les pido disculpas a todos les si bien les dejé. El día jueves si mal no recuerdo si fue el jueves el jueves a la noche creo que fue el PPT con la clase de la semana pasada, mi intención es charlar un poco acerca de eso. Espero que lo hayan podido ver y por más que nada para aquel que que no puedo hacerlo y además para que el que logre yo y tal vez tenga más dudas que Qué certezas la intención es poder revisar algunos de esos conceptos de los que le dejen la clase pasada? Inicialmente todas estas primeras clases de las dos materias.
11:41 Miguel Carlos Pita: Y esta ya sería la última con cuestiones así ya conceptuales o teóricas puras. Ya a partir de la siguiente clase, vamos a empezar a ver si de a poquito cuestiones más prácticas, pero es importante empezar a mencionar algunas cuestiones reflexionar acerca de algunos de algunos conceptos empezar a manejar alguna terminología. Que vamos a ir viendo durante todo el desarrollo de la materia. Sí, pero hay cosas que seguramente ustedes también han manejado.
12:15 Miguel Carlos Pita: Y han trabajado en otras materias, seguro que lo han hecho hoy, vamos a ver y vamos a mencionar algunas técnicas o por lo menos el nombre macro de esas técnicas porque generalmente cuando hablamos de de algunas técnicas también tienen distintas clasificaciones o subclasificaciones, pero bueno, ustedes seguramente han los han visto las han escuchado probablemente han aplicado alguna de ellas. la intención de la clase de hoy es hablar acerca de el pre procesamiento de datos de su importancia mencionar algunas técnicas que se aplican de pre procesamiento Y algunas implicancias o diferencias entre ellas.
13:04 Miguel Carlos Pita: Con el transcurso de las clases siguientes las iremos viendo es más las que vemos en las que vamos a mencionar en la clase de hoy son solo algunas. De todas las que vamos a ver a lo largo de la carrera es más también mi intención. Es que cuando termine la cursada ustedes sepan la oportunidad correcta de ejecución y de aplicación de cada uno de ellos. yo creo que es difícil cuando uno empieza a estudiar estos temas poder organizarse mentalmente acerca de cómo aplicar estas técnicas entonces la importancia de De poder hacerlo de una manera estructurada, el esta materia les va a permitir a ustedes sacarse todas esas inquietudes y todas esas dudas. Esto es de a poquito y paso a paso hay muchos que tal vez quieren.
14:02 Miguel Carlos Pita: Ya estar desarrollando algoritmos súper complejos y me parece buenísimo, pero de eso se lo vamos a tratar de llegar a eso al final de la cursada. Y durante el transcurso de ella, pero lo bueno es que tengamos que empezar a definir algunos conceptos, manejar cierta terminología y para eso están estas. Estas clases iniciales para que podamos todos hablar el mismo idioma así entonces ese es el objetivo de la clase de hoy es simplemente una breve introducción que les quería hacer. Y bueno, ya comparto la pantalla y empezamos a hablar de esto.
14:41 Miguel Carlos Pita: Alguien quiere mencionar algo tiene alguna cuestión? particular que me quiera comentar se ve
15:02 Nazarely Gomez Abularach: Sí, sí se ve.
15:04 Miguel Carlos Pita: Bueno, esto esta es la presentación sí que tienen en el aula virtual, así que vamos a charlar un poco acerca de ella, no sé si la pudieron ver. Si no la vieron a ver esto yo no le voy a decir nada ustedes ya son todos todos grandes entiendo también que la hayan que la hayan leído y que no hayan entendido mucho que hayan entendido entendido una parte.
15:28 Miguel Carlos Pita: Pero bueno, pueden acceder bien a al aula virtual eso no tienen problema, no se han podido notar todos en el aula virtual. Hay alguno que quedó colgado que no tiene acceso o que no sabe que tiene?
15:42 Miriam Coman: Yo soy perdón, yo no encontré el aula virtual directamente.
15:45 Miguel Carlos Pita: la pueden ayudar, por favor algunos si tienen el enlace ahí a mano que se lo pueda pasar a Miriam
15:54 Miriam Coman: Bueno, gracias.
15:54 Miguel Carlos Pita: Si es tan amable, no, por favor, la idea es que todos tengan acceso a lo que vayamos viendo vos estás en la materia de los lunes también.
16:05 Miriam Coman: No, solamente en esta.
16:06 Miguel Carlos Pita: Ah, bueno, bueno. Cuando cuando entres si te pasan ahora el enlace, genial, sino también ahí
16:11 Diego Ferrari: https://aulasvirtuales.bue.edu.ar/course/view.php?id=19438
16:15 Miguel Carlos Pita: están.
16:15 Miriam Coman: Ahí lo pasamos.
16:16 Miguel Carlos Pita: Muchas gracias, gracias.
16:16 Miriam Coman: Ahí me fijo, gracias.
16:19 Miguel Carlos Pita: Dale no, por favor, es es de automatriculación. Sí, así que deberías poder acceder directamente yo todas las clases, le subo las presentaciones. O lo que veamos sí, en caso de que de que codifique algo al finalizar la clase, se los dejo en el aula virtual. Bueno, vamos a charlar un poquito acerca de el pre procesamiento de datos. Tarea por demás importante en la en la carrera en sí, no en esta materia en su carrera. Y más sobre todo cuando están en estas instancias finales.
17:04 Miguel Carlos Pita: debido a que cualquier tipo de análisis que quieran hacer O cualquier tipo de modelo que quieran aplicar va a requerir que ustedes. Le hagan un trabajo a sus datos de pre procesamiento. Estos en principio para que que los datos que ustedes con los que ustedes estén trabajando tengan el formato adecuado para el posterior análisis, sí, ayer en la materia que vemos los lunes, vimos algunos conceptos que ustedes ya entiendo que deberían saber todos ustedes de formato de datos, vimos los formatos de datos estructurados, no estructurados semia estructurados, pero más allá de eso la clase de hoy no tiene relación.
17:53 Miguel Carlos Pita: Con ello si se los quería mencionar lo necesario que se lo sepan, simplemente es que nosotros necesitamos darles a los datos, el formato adecuado no, por
18:00 Miriam Coman: gracias,ya estoy en el aula
18:04 Miguel Carlos Pita: favor, Miriam de nada. Bueno, entonces con el procesamiento de datos buscamos en principio eso llevar los datos un formato para su análisis. en principio para mejorar la calidad de los datos y además que nos permita esta reducción o esta mejora de calidad de los datos reducir mejor dicho la La el costo del análisis de esos datos nosotros ayer hablamos de lo que es la maldición de la dimensionalidad. Yo lo vamos a mencionar.
18:46 Miguel Carlos Pita: El hecho de tener datos de mejor calidad y poder seleccionar los mejores datos va a ser que el costo computacional sea mucho menor y eso es fundamental cuando trabajamos con algoritmos muy complejos que vamos a desarrollar nosotros en esta materia sí. estoy haciendo clic y no Entonces vamos a ver mencionar acá en esta primera diapositiva algunas técnicas de pre procesamiento de datos. Y ya las vamos a ir viendo a todas ellas.
19:19 Miguel Carlos Pita: Creación de atributos estos son algunas hay un montón más selección de un subconjunto de atributos. pre procesamiento de datos mediante agregación mediante normalización mediante muestreo técnicas también de reducción de dimensionalidad técnicas de discretización y vinarización y también transformación de datos también algo Que me parece también importante mencionar es que como yo les decía ayer en la otra materia. En muchas ocasiones ustedes van a encontrar tal vez diferentes nombres.
20:04 Miguel Carlos Pita: Para un mismo concepto sí, entonces en algunos casos se habla de normalización y en otros se habla, por ejemplo de estandarización y estamos hablando en algunos contextos de lo mismo ya vamos a ver esa diferencia igual, pero capaz ustedes dicen bueno, si yo normalizó. Lo que estoy haciendo en realidad es obtener un dato que tenga media de cero y desviación estándar de uno. Pero en otro en otra bibliografía van a encontrar, que eso en vez de normalización es estandarización y le va a mencionar que la normalización es otra cosa que llevar el rango de valores de ese dato de 0 a 1 bueno. Se los menciono simplemente de todas maneras esto lo vamos a ver más adelante.
20:49 Miguel Carlos Pita: Porque muchas de estas cuestiones que probablemente hayan visto en alguna otra materia. Ustedes podrán decir. Ah, esto ya lo vi, pero se llamaba de otra manera o esto lo vi y no tenía este mismo nombre o sí. porque cada maestro con su librito y en función de la bibliografía, que ustedes buscan como Seguramente hacen habitualmente, puede ser que haya algunas diferencias? En algunos conceptos sobre todo en el nombre después el concepto va a ser el mismo lo importante es que ustedes sepan interpretar. Bueno, yo voy a utilizar una una transformación de datos.
21:30 Miguel Carlos Pita: O voy a utilizar una técnica de muestreo o una normalización o una estandarización y el nombre en sí. No tiene mucha importancia lo que tiene importancia es qué hacemos cuando aplicamos ese trabajo ese trabajo de transformación ese trabajo de pre procesamiento el nombre simplemente para este que nosotros sepamos, a qué hacemos referencia, pero lo importante es que se ejecuta cuando lo aplicamos y qué resultados o qué implicancias tiene ese procesamiento en nuestro conjunto de datos, entonces hoy los vamos a mencionar, vamos a hablar un poco acerca de de estos de estas técnicas y luego en la materia iremos viendo los más en detalle, vamos a ir aplicando distintas técnicas de pre procesamiento en conjuntos de datos para ver cómo se aplican, en qué momentos en qué resultados obtenemos? Y podemos de esa manera tener una la información total y cabal de cada uno de estas técnicas en nuestra cabeza luego de haberlas ejecutados, eh? Una cosa es leer, qué hacen y otra cosa es verlo o materializado y ejecutarlo.
22:41 Miguel Carlos Pita: Bueno, entonces una de las técnicas de pre procesamiento es la agregación de datos. Que significa el decir agregar datos combinar dos o más atributos? si bien no vimos en esta materia aún que se me atributo que es un objeto entiendo que a esta altura de la carrera ustedes ya deben saber que si los atributos a lo que hacen referencia y los objetos a lo que hace referencia ayer hablamos un poquito más en detalle de esto en la materia de los lunes, no significa Como les dije a los otros a los alumnos, la mayoría de ustedes que están ya esto lo tienen que saber.
23:25 Miguel Carlos Pita: Un atributo es un concepto esencial de ustedes en esta altura de la carrera a los a los asimismo también que es un objeto cuando combinamos atributos o cuando combinamos objetos en un único atributo o por supuesto objeto, estamos agregando datos haciendo agregación, para qué utilizamos la agregación. Bueno, hay varias ventajas, hay varias ventajas. El principalmente el hecho de agregar datos nos permite. Reducir la cantidad de datos que tiene nuestro conjunto de datos nuestro data set y esto va por supuesto de la mano con la reducción de la dimensionalidad.
24:04 Miguel Carlos Pita: Eso es algo que ayer hablamos un poco hoy más adelante, vamos a volver a mencionar acerca de eso que es la reducción de la dimensionalidad, pero es un factor muy importante que nosotros tenemos que conseguir. Sobre todo porque conjunto de datos muy grandes tienen obviamente ya de por sí de manera intrínseca un costo computacional muy elevado, pero además porque a medida que avancemos en la materia los algoritmos.
24:33 Miguel Carlos Pita: Que vamos a empezar a aplicar, son cada vez más pesados de ejecutarse cada vez requieren mayor carga de ejecución entonces. Si nosotros tenemos un conjunto de datos muy elevado y un algoritmo muy pesado evidentemente eso no es, no es lo más. No es lo más ventajoso y tenemos que tender a conjuntos de datos más pequeños. Sí, una técnica para poder hacer eso es haciendo agregación de datos.
25:02 Miguel Carlos Pita: También para cambiar de escala bueno obtener datos más estables, por ejemplo, si yo estoy sensando con un sensor como bueno, un ejemplo similar a lo que lo que mencionaba ayer estoy tomando un sensor de temperatura que me toma. La temperatura en un lugar del de la ciudad yo lo que puedo hacer es tener varios sensores y obviamente promediar sus valores para obtener un valor de temperatura promedio que evidentemente ese valor promedio va a ser más estable y va a estar ajeno, probablemente a pequeñas variaciones de cada sensor de manera individual entonces la agregación de datos me permite un dato de mejor calidad. Sí, ese es el objetivo que Mencionaba al inicio de de la clase uno de los objetivos del preposición del pre procesamiento de datos es obtener datos de mejor calidad.
25:58 Miguel Carlos Pita: Y bueno, menos es mejor es un poco lo que hacía referencia anteriormente menos tiempo de memoria, perdón, menos memoria y tiempo de preposiciones de procesamiento. Además, los algoritmos también cuando se trabaja con algoritmos más pesados. Es mejor trabajar con menos datos, bueno, es en esta placa trate de materializar un ejemplo. De lo que se podría hacer una agregación de datos la agregación de datos en definitiva, lo que hace es recordemos.
26:35 Miguel Carlos Pita: combinar dos o más datos para obtener un dato nuevo un dato que represente La combinación de esos dos datos anteriores sí, entonces en este gráfico lo que vemos es un diagrama de líneas en el cual se está materializando el porcentaje entre comillas de felicidad en Twitter no entonces en función de las interacciones y de los comentarios ya sea positivos o negativos se establece este diagrama de líneas en el cual los picos positivos.
27:11 Miguel Carlos Pita: Luego de hacer un análisis de este de este gráfico los picos positivos coinciden con festividades, no? Esto está en inglés pero bueno, habla del día de San Valentín de hister, que es Pascuas del Día de la Madre Día del Padre y así hasta Navidad por ejemplo está bien o el Día de Acción de Gracias y los picos negativos se corresponden. Con sucesos tal vez más lamentables o sucesos trágicos algún accidente algún asesinato o alguna cuestión que masivamente tuvo repercusión negativa entonces.
27:54 Miguel Carlos Pita: Que estamos que trato de materializar con este con esta diapositiva, lo que es la agregación de datos, nosotros a través de datos que en este caso van a ser las interacciones de los usuarios. En la red Twitter combinando las interacciones positivas en un periodo determinado voy a obtener un punto en un gráfico y ese punto un gráfico obviamente va a significar. en este caso una combinación de De twitch positivos que en coincidencia va a tener su correlación con un día festivo por ejemplo y lo mismo los valores negativos entonces yo agrego datos. Yo combino esos datos y obtengo un dato nuevo que es una combinación de ellos. Bueno, muestreo hacen.
28:51 Miguel Carlos Pita: Creo que desde desde las primeras materias que han tenido en esta carrera de todas maneras lo vamos a seguir haciendo lo vamos a hacer. Prácticamente todos los conjuntos de datos con los que trabajemos vamos a hacer, vamos a aplicar técnicas de muestreo. Entonces, qué se muestreo la principal técnica de selección de datos? Yo lo que trato de hacer cuando muestreo es a una representar. una población X en una en una muestra Sí, que de tamaño menor sí o como se suele decir de tamaño representativo de esa muestra mayor está bien la muestra que yo tomo tiene que tratar de representar lo más fielmente a la muestra mayor, porque si no estaría cometiendo un sesgo, ya vamos a ver los tipos de muestreo que hay y obviamente los sesgos en los que puedo incurrir si no hago un buen muestreo porque evidentemente con una.
29:55 Miguel Carlos Pita: Con un conjunto de datos de muchas dimensiones o muy grande con muchas instancias, yo puedo. Yo debo muestrear y si no muestreo bien, evidentemente voy a cometer errores ya en el inicio del proceso de mi modelado y esto sirve el muestreo en la investigación preliminar o en la investigación final como bien, dice ahí. Es simplemente para ejemplificar si yo por ejemplo, quiero mostrar una población de un millón de habitantes. Y yo hago inicialmente una un muestreo preliminar. Con una muestra menor, por ejemplo muestreo a mil personas.
30:43 Miguel Carlos Pita: O así en personas entonces yo mediante ese muestreo puedo analizar? Cuestiones iniciales en las cuales puedo adelantarme a posibles problemas, que voy a tener luego al ejecutar mi modelo entonces puedo corregir ciertas o puedo, por ejemplo también identificar si esa esa muestra va a ser conveniente de ser analizado, no? Y en caso de que yo decida mediante ese muestreo preliminar de que esa muestra de que esa muestra no de que ese conjunto de datos es viable ser analizado yo comienzo con mi modelado con una muestra final que va a ser más representativa de ese conjunto.
31:25 Miguel Carlos Pita: Datos en vez de muestrear a 100 personas mostraré a 10.000, sí, si estoy analizando un conjunto de datos de un millón de personas entonces esa es la diferencia entre lo que sería la investigación preliminar o final la preliminar se hacia el inicio me permite adelantarme a posibles problemas. Y la final se hace cuando ya decidido finalmente trabajar con ese conjunto de datos que mostrarlo con una muestra que sea representativa y que trate de asemejarse lo más fielmente posible a la muestra general a la muestra general no al conjunto de datos general.
32:03 Miguel Carlos Pita: Bueno, se usa el muestreo se usa en estadística y por supuesto en datamining en minería de datos. Lo vamos a usar continuamente en esta materia. bueno, cuando es efectivo eso vamos a ver ahora las distintos tipos de muestreo y cuando hay que utilizar cada uno de ellos o cuando O las diferencias entre ellos para saber cuándo conviene aplicarlos, sí tenemos el muestreo aleatorio. Qué ventajas tiene el muestreo mostrar aleatorio así como bien dice la palabra yo.
32:38 Miguel Carlos Pita: Avedo en realidad no lo voy a hacer suadero, no sí hay algoritmos que hacen muestreo aleatorios. Pero se entiende la explicación, sí, nosotros seleccionamos arbitrariamente distintos distintas. distintos puntos distintos datos de nuestro conjunto de datos entonces estoy de manera aleatoria seleccionando esa muestra Que puede pasar con un muestre aleatorio primero evita que incurran sesgos. Sí, evita, que encurrejos el hecho de que sea aleatoria la muestra. Y no seleccionar, por ejemplo, los primeros 10 o los últimos 10.
33:19 Miguel Carlos Pita: Por poner un ejemplo, pero también muestra oratorio, me puede llevar algunas desventajas? Una de ellas es que la muestra seleccionada no sea representativa. No represente fielmente las mayores características o las especificidades de del conjunto de datos inicial o con el que estoy trabajando y además un problema, que vamos a tratar de evitar nosotros a lo largo del desarrollo de esta materia que es el problema del desbalance.
34:00 Miguel Carlos Pita: que que algunos Que se puede explicar qué significa o qué le parece que es el desbalance entre conjuntos de entrenamiento y prueba seguramente lo sepan por las dudas algunos que tal vez entienda que hacemos referencia con eso. Ya sea mediante un ejemplo. Bueno, qué es el desbalance?
34:30 Ricardo Nuñez: Profe consultar acá entrenamiento y pruebas son lo mismo.
34:32 Miguel Carlos Pita: Sí, dale, dale. No, no son lo mismo, no, no, no son lo mismo ya lo vamos, ya vamos a ver esa
34:39 Ricardo Nuñez: ok
34:42 Miguel Carlos Pita: diferencia. Ya lo vamos a ver y vamos a hablar un montón de esto está buenísima tu pregunta porque ya vamos a establecer esa diferencia. entre entre ambos conjuntos que son fundamentales en muchos casos Hay gente que modela sin dividir el conjunto de datos en entrenamiento y prueba. Lo cual es un error de modelado a mi entenderse. Ya vamos a ver cómo hacer esa división en el conjunto de datos en conjuntos de entrenamiento y prueba y cómo analizarlos. En para explicarlo de manera breve tal vez. Y que se pueda entender nosotros cuando creamos desarrollamos un modelo.
35:36 Miguel Carlos Pita: Ya sea de clasificación de regresión todos esos conceptos los vamos a ver más adelante. El conjunto de datos tenemos que dividirlo en un conjunto de datos de entrenamiento y un conjunto de datos de prueba el conjunto de datos de entrenamiento vamos a tener una parte de ese del conjunto de datos generales. Con el con un atributo que se llama atributo clase incorporado el conjunto de datos de prueba va a tener la otra parte del conjunto de datos, pero sin el atributo clase.
36:11 Miguel Carlos Pita: Incluido lo que nos va a permitir el conjunto de entrenamiento como su palabra misma lo explica es que nuestro modelo sea entrenado es decir el nuestro modelo va a fijarse. Todos los atributos de un conjunto de datos y las relaciones entre ellos y cuáles son los patrones que llevan a esas instancias a tener una clase determinada, por ejemplo cuando hablo de una clase determinada puede ser por ejemplo.
36:42 Miguel Carlos Pita: Si es recomendable otorgar un crédito o no a un a una persona en un banco entonces en mi conjunto de datos voy a tener que un postulante para un crédito tiene su edad su trabajo el trabajo el que se dedica sus ingresos mensuales si tiene deudas o no y un montón más pueden haber entonces el atributo clase me va a decir. Si es factible que se le entregue un crédito o no, entonces qué es lo que hace mi modelo se entrena? Para entender los patrones y las relaciones entre esos atributos y poder determinar y predecir a futuro.
37:26 Miguel Carlos Pita: De mejor manera a los futuros posturantes para que se les entreguen créditos y qué hace el algoritmo se hace una prueba con el conjunto de pruebas, eso lo vamos a hacer nosotros. Nosotros al conjunto de prueba, lo vamos a utilizar para probar, cuánto es las actitudes y la precisión que tuvo nuestro modelo luego de ser entrenado con el conjunto de entrenamiento y yo lo que voy a terminar.
37:53 Miguel Carlos Pita: Haciendo es comparando los resultados que fueron predecidos por mi modelo con las etiquetas que tenía nuestro conjunto de pruebas y hacer al hacer esa comparación nos vamos a dar cuenta, si nuestro modelo está prediciendo bien o no se entrena con el conjunto de datos de entrenamiento y pruebo la precisión del modelo con el conjunto de datos. El de prueba está buena buenísima la pregunta porque no son lo mismo y lo vamos a ver, vamos a ver cómo diferenciar ambos y lo vamos a utilizar un montón de hecho, es una de las primeras etapas. Luego de hacerla luego de hacer la limpieza de los datos de hacer un montón de técnicas previas de procesamiento, pero una de las primeras cosas que hacemos, es dividir los datos en conjunto de entrenamiento y prueba entonces.
38:47 Miguel Carlos Pita: Bueno, qué significa el desbalance? Vamos, volvemos entonces a lo que es el desbalance. El desbalance significa que yo si hago un muestreo aleatorio. Lo que puedo ocurrir es que en mi conjunto de entrenamiento, por ejemplo, vamos, volvemos al ejemplo del crédito. Me viene bien ese ejemplo para poder explicar esto si volvemos al ejemplo del crédito. Cuando es una clase cuando hay una clase desbalanceada, si yo mi conjunto de entrenamiento tengo 95 instancias con vamos, un ejemplo que tengo 100 objetos o 100 instancias en mi conjunto de datos y 95 de ellos.
39:27 Miguel Carlos Pita: Había que darles un crédito sí, a sólo un 5% de ese conjunto, perdón, cinco solo objetos de ese conjunto de entrenamiento. No había que darles la etiqueta de esos era que no se les tenía que entregar un crédito cuando se entrena el modelo. Ese desbalance lo que va a hacer es que los patrones entre los datos. Como la muestra no es representativa de lo que puede suceder en la realidad de que pueda haber igual cantidad de personas que necesiten el que se les pueda dar un crédito como igual cantidad de personas que no probablemente nuestro modelo y nuestro algoritmo termine.
40:05 Miguel Carlos Pita: Aprendiendo que en general a la mayoría de la gente hay que darles un crédito sin tener en cuenta algunos patrones que pueden hacer que eso no tenga, no deba ejecutarse sí, entonces cuando yo tenga que probar. Mi modelo probablemente a la mayoría de los datos del conjunto de prueba. mi modelo va a decir, sí, hay que entregar un crédito pero Tuvimos un sesgo tuvimos un inconveniente que no fue tenido en cuenta que es el desbalance entre clases, probablemente en ese conjunto de prueba.
40:38 Miguel Carlos Pita: Hay otros factores a analizar y mi modelo no los va a ver aprendidos ya que el conjunto de datos estaba desbalanceado cuando se hizo el entrenamiento, sí. No sé si quedó claro, es complicado tal vez explicarlo y no. No, no, no se valora la práctica, pero en cierta forma trate de ser lo más.
40:58 Ricardo Nuñez: Ahí consultó de nuevo, por ejemplo, tengo 100 personas, hay 10 personas que
41:00 Miguel Carlos Pita: Sí, dale, dale.
41:05 Ricardo Nuñez: que evidentemente no tenían que recibir el crédito por X motivos muy claros. O es una sola persona entre 100 que por motivos muy claros no tenía que recibir el crédito, o sea, está totalmente desbalanceado.
41:16 Miguel Carlos Pita: Bien.
41:19 Ricardo Nuñez: Si está entrenado de esa forma y después. Podría ser que una persona con esas características le terminan dando un crédito porque se desbalanceó, eso es lo que por ahí no me queda muy claro.
41:31 Miguel Carlos Pita: claro, es probable, por ejemplo, vos tenías ese ejemplo de 100 y había una 99 personas y una que no, entonces tu modelo en general lo que va a aprender es que Porque la mayoría de los patrones que analiza es que sí, es que las personas Salta no es un crédito porque generar análisis al analizar al interrelacionar. los los distintos atributos no va a tener ejemplos concretos de personas a las que no se le hayan dado el crédito y no va a poder hacer un correcto análisis de cuáles son los factores que influyen en esa decisión entonces sí Hernández vale
42:08 Hernán Hamra: No me supongo, es una consulta que cuando uno se para aleatoriamente una una muestra, supongamos que tenga una distribución específica de edad y que esa sea una característica importante para para la muestra final si aleatoriamente.
42:24 Miguel Carlos Pita: Disculpe un poco mal, perdón. Te estaba escuchando un poco mal, no, no te entendía al principio.
42:31 Hernán Hamra: No, que me imagino que la autoridad lo que hace es romper cierta característica de la población, por ejemplo, si si es importante la edad de la población y en la aleatoriedad de esa muestra construyó, otro tipo de población de distribución de edad. Entonces me estaría afectando. Me imagino que será eso, no sé si es la pregunta si es esa.
42:53 Miguel Carlos Pita: Por supuesto que sí podés tener suerte y que la peguen justo y la aleatoriedad.
42:59 Hernán Hamra: Y y que de esa forma, ahí habría un desbalance entre la muestra y y la prueba
43:01 Miguel Carlos Pita: pero bueno
43:06 Hernán Hamra: sería eso.
43:08 Miguel Carlos Pita: totalmente por eso después suelen haber otras técnicas, no sé si te terminé de contestar perdón Ricardo yo creo que igual lo que vos me habías mencionado, es es así como Este creo que no sé si te dijera la respuesta.
43:20 Ricardo Nuñez: Ahí con el comentario de compañero, me quedo más claro, porque entonces claro, por ejemplo, agarra y de 50 de lo que aprobó y justo en ese estaba ese que está mal, no por ejemplo y son de no sé del barrio de La Boca después se empieza a entender que porque son de la boca hay que darles un crédito algo por el estilo en vez por ahí pondré otras cosas.
43:38 Miguel Carlos Pita: Exacto sin duda no, pero por por un ejemplo, si es es así es así lo que vos tenés que tratar es que, o sea lo ideal, sería que tu algoritmo se entrene con todos los datos disponibles y con la mayor cantidad de información posible lo que pasa que eso siempre tiene cuestiones contraproducentes, entonces juegan tiempo juega en contra el tiempo de procesamiento y el tiempo que le va a demorar a tu algoritmo.
44:05 Miguel Carlos Pita: hacer el aprendizaje entonces y aparte no solamente es el tiempo el tiempo del algoritmo, en sí es todo el tiempo de pre procesamiento que a vos te va a llevar limpiar los datos que sean de buena calidad para poder comenzar a ejecutar los algoritmos de aprendizaje que necesitas son varias cuestiones que van atrás de esto sí, entonces lo importante es que empecemos a tener en cuenta estas cuestiones sí, y que siempre demos Aleatorio no es la mejor no es la mejor decisión, después vamos a ver algoritmos, que ya algo esto hablamos ayer, pero que le dan peso a los atributos, entonces yo voy a saber cuáles son los los atributos que más peso tienes más peso tienen y que más pueden influir en el resultado final que yo necesito obtener y entonces voy a tratar de seleccionar ese tipo de atributos.
44:56 Miguel Carlos Pita: Entonces vamos a ir viendo un poco todas esas técnicas. Bueno, seguimos. Otro tipo de muestreo es el muestreo estratificado. Qué hacemos todo certificado lo que hace es a sus poblaciones? De la muestra les trata de representar aleatoriamente sí, entonces yo tengo una muestra con su población es y voy a tratar de que cada una de esas poblaciones están representadas en la muestra que estoy obteniendo sin dejar a ninguna afuera.
45:30 Miguel Carlos Pita: Y hay distintas maneras de de hacer esta estratificación? Si es en estadística. El trabajo se hace de manera que cada sub población. Tenga el mismo porcentaje en la muestra final. Que el porcentaje original que tenía en la en el conjunto de datos, total, sí, para representar fielmente o de la mejor manera posible a esa sub poblaciones en la muestra que estoy obteniendo, pero esto viene también un poco en referencia a lo que hablábamos del desbalanceo de clases, que es algo que nosotros tenemos que evitar hacer.
46:14 Miguel Carlos Pita: Y lo vamos a mencionar varias veces y vamos a ver cómo cómo compensar esa ese desbalance. en minería de datos Nosotros tenemos que tratar de equilibrar las poblaciones está bien por más que en la muestra que estoy tomando. Sean sus poblaciones menores a otras yo en la en la muestra que estoy tratando, que estoy obteniendo final tengo que equilibrar esa subpoblaciones. Esto viene en relación con lo que es el balanceo de clases que habíamos mencionado anteriormente sí, entonces si yo quiero obtener un crédito trato de llevar al 50 y 50 las posibilidades para que nuestro modelo aprenda con la mayor cantidad de información posible ambos extremos sí de la muestra eso se llama cuando yo quiero sobre representar una población menor a otra se llama oversempling, sí, sobre representar se conoce como over-sampling yo una muestra pequeña.
47:17 Miguel Carlos Pita: La este agrando para sobre muestreo mejor dicho para equilibrarla con una muestra mayor. muestreo sistemático esto Lo que hay que hacer previamente es ordenar una población y luego de estar ordenada. Se obtiene una muestra a intervalos regulares, pero la particularidad es que el punto de partida. Tiene que ser aleatorio. Entonces yo tengo una muestra general. Arranco desde un punto de partida aleatorio que en este caso puede ser el 2 y a partir de allí tomó las muestras.
48:01 Miguel Carlos Pita: a través de intervalos regulares, si esto es más que nada para que conozcamos los distintos tipos de De muestreo que tenemos, hablamos de muestreo de muestreo y parece que hay una sola forma de hacerlo y no hay muchas maneras. Muestreo proporcional que es bueno es muy simple no es muy similar, es lo mismo que habíamos hablado anteriormente lo que se conoce como muestro proporcional es, ya sea el over sampling o el subsampling. Sobre representar una población pequeña y si hago lo inverso. Que es que una población si yo tuviese.
48:42 Miguel Carlos Pita: Un millón de casos que sí y mil que no por poner un ejemplo si yo llevo a ese un millón a mil para equilibrarlo con la otra muestra. Estoy hablando de subsamplings, sí, la población muy grande, la disminuye para equilibrarla con otra población o con otra sub población creo que se entiende el concepto. Bueno, entonces, qué es lo importante? De todo el tema del muestreo que estamos hablando que el tamaño de la muestra.
49:20 Miguel Carlos Pita: Que se toma es importante sí o tiene injerencia en el modelo en el en el gráfico que están observando, estamos viendo un mapa de puntos entonces nosotros podemos apreciar que si tengo una muestra muy grande de 8000 puntos puedo reconocer en esa imagen un patrón bien definido. Y que ese patrón que estoy analizando, que estoy observando en este. En este gráfico con 8000 puntos demuestra si empiezo a reducir el tamaño de la muestra que se toma dejo de reconocer determinados patrones y lo que en este gráfico significa un patrón que pueden ser ese dibujo que se observa medio geométrico o con forma de intercalando.
50:11 Miguel Carlos Pita: En un conjunto de datos lo que van a representar eso eso esos puntos o estos patrones que se observan son las relaciones entre los datos y es lo que realmente caracteriza. A mi conjunto de datos si yo lo muestreo y lo muestreo de una manera muy pobre con una con un tamaño de la muestra muy pequeño. Por más que esté reduciendo la dimensionalidad también estoy cometiendo un error ya, que no voy a estar teniendo toda la información necesaria o a disposición para caracterizar fielmente mi conjunto de datos. Entonces lo que se observa en este gráfico.
50:49 Miguel Carlos Pita: que que creo que a simple vista es bien entendible, se tiene que traspolar a cualquier tipo de información o de conjunto de datos no hace falta, no hace falta que sea un mapa de puntos con un patrón de visualización, estamos hablando de un conjunto de datos con patrones que como nosotros como seres humanos no podemos apreciar a simple vista, pero si se si el tamaño de la muestra es muy reducido, esos patrones se pierden porque las relaciones entre los Atributos y entre las características de las distintos objetos o de las distintas instancias de mi conjunto de datos pierden sentido dejan de tener relaciones entonces mi modelo cuando tenga que analizar estos patrones no los va a encontrar.
51:29 Miguel Carlos Pita: Entonces hay que encontrar un equilibrio entre entre el tamaño de una entre un tamaño de muestra muy pequeño. O un conjunto de datos muy elevado, este es un ese balance del que yo les hablaba ayer es fundamental. Porque es gran parte del éxito de que nuestro modelo sea bueno y de que prediga bien y que tenga una baja tasa de errores. Bueno, esto es un poco mencionar lo mismo se los se los comento rápido, pero no no es para darle muchas vueltas es un poco más de lo que veníamos hablando.
52:11 Miguel Carlos Pita: la importancia del conjunto de del tamaño de la muestra si la importancia del tamaño de la muestra en En el muestreo entonces acá se pregunta yo tengo 10 grupos. Si tengo 10 grupos dentro de cada uno de estos grupos. Hay información hay datos, hay objetos lo que ustedes lo que ustedes se les ocurra lo importante es que acá hay 10 grupos que presentan. objetos en su interior entonces si yo tuviese que Si yo te si yo puedo tomar solamente de manera aleatoria o yo puedo sacar 10 objetos nada más en total.
52:54 Miguel Carlos Pita: si lo que lo que este gráfico está representando es Si yo tomo una muestra de 10 puntos únicamente la probabilidad la probabilidad de tomar uno de cada uno de estos grupos que solamente sea uno de cada uno es prácticamente el 0%, es probable que si yo tengo una muestra aleatoria, no tome uno de cada uno de estos puntos sí capaz como dos de este tres de este uno de este dos de este, entonces algunos grupos no van a ser representados en la muestra final entonces la probabilidad de obtener al menos un objeto de cada uno de los diez grupos tiende a cero tal vez tengo suerte y bueno es a uno en un millón la pegué, pero suele ser esto en minería de datos se trabaja con con estadísticas y con probabilidades, ya vamos a ver algunas de esas cuestiones.
53:48 Miguel Carlos Pita: Entonces si yo tomo solamente 10 muestras la probabilidad tiende a cero y a medida que voy aumentando el tamaño de las muestras, que voy tomando empiezo a Uno que el uno sería el 100% en el caso de ya con 60 muestras estoy pudiendo representar al menos uno un objeto de cada uno de estos 10 grupos. Esto es más que nada para Mencionar la importancia del tamaño de las muestras sí, la idea es encontrar un punto un equilibrio, si no irme muy no tomar mil muestras si ya con 70 puedo representar fielmente.
54:29 Miguel Carlos Pita: toda la todo el conjunto de datos Bueno, y todo esto que veníamos hablando, es la introducción a algo que hacer algo, ya habíamos hablado y que yo creo que ya se los comenté anteriormente acerca de la maldición o cure of dimensional o maldición de la dimensionalidad. Que evidentemente lo que me está hablando es de que la dimensión me va a traer un problema, ya sea. muchas dimensiones o pocas dimensiones el equilibrio es Buscar un punto medio en el cual no tenga muchas dimensiones que que lo que hagan es que el tiempo de procesamiento sea muy elevado con un costo con un costo computacional enorme y con una eficiencia muy pobre y obviamente también no pocas dimensiones bajo las cuales evidentemente.
55:30 Miguel Carlos Pita: Mi modelo no va a poder representar fielmente las características de mi conjunto de datos hallar correctamente los patrones y las características que los representan no va a poder hacer un aprendizaje correcto de esas relaciones entre el conjunto de datos debido a que tengo pocas dimensiones y eso va a repercutir en que haga malas predicciones, mi modelo, sí. Entonces la idea es buscar un equilibrio entre ambos puntos para eso es importante seleccionar una buena muestra con un buen tamaño de muestra para una buena muestra representativa de mi conjunto de datos.
56:06 Miguel Carlos Pita: También si yo tengo datos un conjunto de datos con muchos atributos tampoco es bueno ya que si aumenta la dimensionalidad. Bueno, estos estos simplemente a título informativo en general, si yo tengo muchas dimensiones. Los datos pierden también con mucho cuando tengo muchas dimensiones y esto viene relación con algo que hablé ayer del tema de la resolución, sí. El tener muchas dimensiones hace que los datos al estar.
56:38 Miguel Carlos Pita: Muy espaciados entre sí, o sea, las nociones de lo que son las distancias entre los entre los datos es decir sus características sus diferencias. Más notorias entre sí pierden sentido sí, al haber tantas dimensiones realmente los datos que tal vez me están aportando valor pierden. ese valor que tienen innato intrínseco en sí, porque terminan siendo prácticamente todos lo mismo esto es el tema de la resolución que hablábamos ayer si yo tengo una Una baja resolución sí, una baja resolución. Yo voy a perder.
57:19 Miguel Carlos Pita: características de cada uno de los datos en cambio si yo tengo una alta resolución Yo voy a poder cuando estamos hablando de una alta resolución era por ejemplo si yo observaba los datos de temperatura. De en una hora y yo los observo segundo a segundo entonces yo ahí tengo una alta resolución, puedo determinar cómo van variando los datos segundo a segundo en cambio, si yo tengo el valor de temperatura en una hora lo tengo distribuido cada media hora, voy a tener solamente dos puntos que analizar y evidentemente ahí pierdo resolución. Yo no sé qué pasó en un cuarto de hora o a los 10 minutos. Bueno, acá es algo parecido cuando yo extiendo las dimensiones los datos pierden los datos pierden información entre sí, entonces yo necesito menor cantidad de dimensiones.
58:13 Miguel Carlos Pita: Y lo mismo pasa con la densidad la densidad habla de cuán tan próximos o más cuán separados están los los datos entre sí, entonces con muchas dimensiones pierden significado ese tipo de conceptos hay que tratar de evitar tener muchas dimensiones. Sumado a lo que es el tiempo de procesamiento y todo eso que ya venimos hablando, no? bueno, vemos esta placa y hacemos un unos minutitos de De intervalo, entonces, cuál es el propósito de reducir la dimensionalidad evitar la maldición de la dimensionalidad en principio que vamos a ver reducción de la mencionalidad son las técnicas que vamos a aplicar para reducir dimensiones en un conjunto de datos, el principal es evitar lo que hablamos recién la maldición de la dimensionalidad. Además, el tema de los costos o el tiempo de procesamiento el tiempo.
59:10 Miguel Carlos Pita: de ejecución de nuestros algoritmos los costos computacionales También la memoria no obviamente la memoria que lo cual no no deja de ser importante. Otra cuestión extremadamente importante cuando nosotros tenemos muchos datos perdón muchas dimensiones. Perdemos y queremos por ejemplo observar en un gráfico, cómo es la relación entre esas dimensiones? Obviamente yo no puedo graficar 20 dimensiones. Sí, entonces yo en mi cabeza. Con de ser humano que solamente entiendo dos y tres dimensiones pierdo interpretabilidad de lo que de cómo esos datos están relacionados entre sí entonces las técnicas de reducción de dimensionalidad permiten.
59:58 Miguel Carlos Pita: Que nosotros por ejemplo, llevemos un conjunto de datos de 20 dimensiones a tres dimensiones y que luego yo pueda observar gráficamente cómo están relacionadas las distintas instancias desde ese conjunto de datos, ya vamos a vela algoritmos que reducen la dimensionalidad y que luego nos permiten, por ejemplo, visualizar en clusters, los conjuntos de datos, entonces yo puedo interpretar. Ah, este conjunto de datos pertenece a este cluster y lo veo graficado en una nube de puntos con un gráfico de dispersión separado de otro cluster. Yo creo que han visto seguramente ustedes camins o ese tipo de algoritmos y si no los vieron vamos a ver y los vamos a ver graficados, entonces cuando reducen dimensionalidad pueden empezar a interpretar mejor su conjunto de datos.
01:00:49 Miguel Carlos Pita: Inclusive cuando ustedes lo grafican, vamos a ver un montón de técnicas de graficación y vamos a ver cómo cambiarle color a los clusters para que ustedes también no solamente los tengan separados gráficamente en el espacio en dos o en tres dimensiones, sino que también pueden aplicarles paletas de colores o distintos o distintas formas a las nubes de puntos, entonces ustedes van a interpretar mejor lo que su conjunto de datos representa y la interpretabilidad de su conjunto de datos es fundamental.
01:01:20 Miguel Carlos Pita: Es fundamental porque ustedes interpretando su conjunto de datos van a tener mejor criterio de decisión a la hora de tener que aplicar un modelo. Entonces todo esto que yo estoy hablando es es algo que todas las clases a medida que vamos paso a paso avanzando en los distintos algoritmos, lo sigo mencionando, lo seguimos charlando y lo digo más que nada y se los repito una y otra vez para que entre como este.
01:01:46 Miguel Carlos Pita: Como hacen como hacen los padres con los hijos que les repiten 20 veces, anda la bate los dientes anda a hacer esto anda hacer lo otro bueno, terminan haciéndolo por repeticiones. Entonces ustedes a medida que vayamos charlando de estas cuestiones, ya van a poder interpretar aquellos que aún no lo hacen todas. Estas toda esta terminología y todas estas cuestiones que no le sean ajenas a ustedes y que no piensen que esto es imposible o que es súper difícil no es difícil, les puedo asegurar que no es difícil y que vamos, vamos al final de la cur. A tener un muy buen nivel para hacer modelos de minería de datos bueno.
01:02:25 Miguel Carlos Pita: Además, lo que es la reducción de la dimensionalidad, el hecho de quitar atributos y relevantes or ruidosos obviamente si yo mi conjunto de datos. A través de mi conocimiento que era lo que les mencionaba o de la interpretabilidad que hablaba anteriormente de mi conjunto de datos que para eso tengo que saber qué significa cada atributo. Qué significa cada objeto o qué representa cada objeto o cada distancia en el conjunto de datos? Yo puedo tomar una decisión decía un atributo, no me sirve y lo puedo eliminar o si tengo ruido en los datos que obtuve si puedo eliminar algún atributo y el hecho de eliminar esos atributos hace que se reduzca la dimensionalidad.
01:03:07 Miguel Carlos Pita: Y obviamente va a ser ventajoso porque esos datos con ruido o datos que son irrelevantes, por ejemplo, datos duplicados datos que tienen valores faltantes, hay cuestiones, que ya hemos visto ayer, pero se los mencionó y lo vamos a seguir charlando esos datos no me aportan ningún tipo de información a mi modelo para que pueda predecir a futuro lo que yo necesito entonces es necesario sacarlo y el hecho de eliminar eso.
01:03:30 Miguel Carlos Pita: Hace que la dimensionalidad sea reducida. Bueno, ya hablé mucho. Me voy a tomar, vamos a tomarnos 10 minutos y Y y cuarto seguimos les parece alguien quiere decir algo.
01:03:44 Hernán Hamra: Profe tal vez no esté en esta segunda parte de la clase. Después lo veré grabado. Que algún compañero lograba.
01:03:51 Miguel Carlos Pita: Perdón, no te entendía hermano, se corta, pero cuando hablas, no, no?
01:03:56 Hernán Hamra: Así que voy a no voy a poder estar en esta segunda parte de la clase.
01:03:59 Miguel Carlos Pita: nada, no te preocupes, no, no voy a terminar de
01:04:02 Hernán Hamra: Bueno, perfecto, seguramente veré grabar la clase que algún compañero.
01:04:04 Miguel Carlos Pita: Voy a terminar de ver este de dar la presentación que tienen en el powerpoint de todas maneras, si tenés alguna duda me mandas un mail, si vos hay algo que no entendés de lo que sigue o lo charlamos el martes que viene no hay problema. Ah, bueno, dale perfecto, listo mejor entonces.
01:04:25 Hernán Hamra: Bueno, muchas gracias, saludos.
01:04:28 Miguel Carlos Pita: Chao gracias, bueno, nos vemos 21 a 15, por favor, sí.
01:15:49 Miguel Carlos Pita: Bueno, estamos seguimos un poquito más, cómo están?
01:16:01 Deisy Rios: Hola, profe, va a volver a pasar lista porque llegué justo cuando termine de
01:16:03 Miguel Carlos Pita: Si ya te anotó.
01:16:05 Deisy Rios: pasar. ríos
01:16:09 Miguel Carlos Pita: Porque sí, sí, sí, te vi, igual te vi después. Sé que entraste ahí te pongo presente, gracias por hacerme acordar.
01:16:21 Andrea Alvarez: Sí, gracias.
01:16:21 Miguel Carlos Pita: aprovecho Álvarez dale Andrea no, por favor y me estaría faltando Gorosito la abuela Michelini bueno Bueno, seguimos un poquito más con lo que resta de lo que es esta. Esta presentación me parece que estos temas están buenos que los vayamos charlando, se va entendiendo. Algunos está muy perdido como vienen en general, vienen bien.
01:17:16 Martín Badino: Sí, profe.
01:17:19 Miguel Carlos Pita: bueno
01:17:20 Miriam Coman: Si se entiende bien.
01:17:22 Miguel Carlos Pita: perfecto bueno Vamos a compartir. Bueno, reducción de la dimensionalidad. Esto ya lo vimos, se ve se ve bien o
01:17:42 Emilce Alconz: Sí, se ve bien.
01:17:42 Miguel Carlos Pita: Pueden mirar buenísimo, gracias. Bueno, en este ejemplo. trato de representar lo que sería la conveniencia de de Reducir la dimensionalidad estos ayer hice un ejemplo similar tal vez de explicado de otra manera, pero el concepto es el mismo sí. Nosotros tenemos un documento que tiene una serie de palabras y si yo quisiese perdón, si yo quisiera Analizar las la cantidad de veces en que las palabras son repetidas esta sería la frecuencia de repetición de las palabras, lo que está en el eje en el eje X o en el eje de las abscisas si en el eje y o en el eje ordenadas, yo tengo cada una de las palabras y bueno las barras me representan la frecuencia que cada palabra aparece en ese documento o en ese conjunto de documentos, entonces hay algoritmos que me podrían llegar a analizar.
01:18:48 Miguel Carlos Pita: A qué tipo de documento estamos haciendo referencia en este caso como podemos ver las primeras palabras corresponden a conectores, sí? De off to y recién una de las primeras palabras es plant, está bien. Entonces probablemente sea algún estoy, estoy inventando, no puede ser que sea un documento relacionado con botánica o algo similar. Habla de genes, bueno, habla nuevamente de plantas aquí abajo. Pero lo importante es ver esto sí que las primeras palabras son conectores por ende, yo está en los conectores en general los tengo que sacar del análisis, porque no me aportan ninguna información relevante para este tipo de análisis, que estoy haciendo de tratar de determinar, a qué tipo de documento como el estilo del documento el cual estamos haciendo referencias.
01:19:49 Miguel Carlos Pita: Entonces bueno, lo que es el gráfico, no no tiene mucho mucha importancia. Mencionarlo ahora entonces lo importante es reducir la dimensionalidad siempre y cuando dependiendo del contexto ya ya les había explicado que en el caso de que estuviese definiendo el idioma del texto el idioma del texto, por ejemplo, si yo quiero saber si está escrito en inglés o está escrito en francés o en alemán evidentemente. Tal vez los conectores por más que se repitan muchas veces son importantes en el análisis y en ese contexto no sería necesario sacarlos.
01:20:29 Miguel Carlos Pita: Pero bueno, lo importante es que ustedes vean la importancia de reducir la dimensionalidad y que eso también depende del contexto en función de el análisis, que estoy haciendo y que estoy necesitando. bueno Cuáles son las diferencias entre reducir la dimensionalidad y hacer lo que se llama selección de atributos seleccionar atributos? Ambos sirven para lo mismo que es para reducir la cantidad de atributos, sí, cuando hablo de selección de atributos es yo tengo un conjunto de datos y me quedo con los que a mí entender son los mejores, me quedo con este, me quedo con este, me quedo con este otro. Los dos métodos las dos técnicas sirven para reducir la cantidad de atributos. Bueno, vamos a ver las diferencias entre ellos.
01:21:17 Miguel Carlos Pita: Cuando reduzco la dimensionalidad generalmente se aplican técnicas de álgebra lineal, no es que nosotros la vamos a aplicar. Nosotros vamos a aplicar un algoritmo que detrás aplica técnicas de álgebra lineal de lineal para tratar de hallar, las combinaciones lineales entre los distintos atributos está bien, por eso es importante que el tener muestras bien representativas del conjunto de datos en general, pero la diferencia es esa reducir la dimensionalidad implica.
01:21:52 Miguel Carlos Pita: La aplicación en el trasfondo de técnicas de perdón técnicas de álgebra lineal, una de esas técnicas una técnica muy conocida no sé si la han visto la vamos a ver en esta materia es PCA que es el acrónimo de principal componente análisis Análisis de Componentes Principales que me permite reducir la dimensionalidad. analizando combinaciones lineales entre los atributos Y yo de esa manera a un conjunto de datos de 20 dimensiones lo voy a poder transformar en un conjunto de datos de cinco dimensiones o de cinco componentes principales por eso el nombre es principal componer análisis. Ya lo vamos a ver. Por otra parte lo que es la selección de atributos.
01:22:42 Miguel Carlos Pita: Implica que yo elija los atributos que van a ser utilizados para el posterior análisis y de esa manera lo que se van a hacer es eliminar en ese en ese proceso las columnas menos menos significativas o las que menos peso tienen sí, por eso habla de porciones de datos innecesarios, pero cuál es la diferencia? fundamental entre una selección de atributos y una reducción de dimensionalidad que si yo selecciono atributos Los atributos que me terminan quedando en mi conjunto de datos son atributos que yo conozco. Vamos a poner un ejemplo, yo tengo un conjunto de datos.
01:23:25 Miguel Carlos Pita: de de un libro Y ese libro estoy analizando el autor estoy analizando el año de publicación estoy analizando donde se vende si es de tapa dura si es de tapa blanda, estoy analizando la cantidad de ejemplares que fueron editados por poner algunos ejemplos no, eso serían los atributos de los distintos objetos de mi conjunto de datos que serían los libros. Entonces si yo quiero analizar o ejecutar un modelo desarrollar un modelo que prediga si ese libro va a ser un best seller o no. Si yo selecciono atributos, por ejemplo, puedo establecer y miren.
01:24:15 Miguel Carlos Pita: nuevamente estoy inventando, no tal vez el año de publicación no es significativo, lo voy a sacar tal vez la cantidad de páginas que tiene el libro no es significativo, lo voy a sacar y así elijo tres o cuatro atributos los cuales para mí no son importantes o más también puede ser pero los atributos con los que Con los que yo me quede son atributos que yo conozco y que sé qué significan, por ejemplo, quién fue el autor que lo escribió cuál es el género del libro? Si es de suspenso, si es de terror si es de una novela.
01:24:49 Miguel Carlos Pita: En definitiva, yo puedo interpretar el significado del conjunto de datos con el que estoy trabajando y de los atributos cuando hago selección de atributos cuando reduzco la dimensionalidad mediante técnicas de álgebra lineal, los componentes principales son combinaciones lineales. De los atributos originales por ende yo pierdo interpretabilidad está bien entonces si tengo que ejecutar posteriormente técnicas. De eliminarte atributos o si tengo que ejecutar técnicas inclusive para yo saber los resultados.
01:25:27 Miguel Carlos Pita: del del hecho de haber ejecutado esa técnica como PSA yo no voy a saber qué significa el primer componente principal el segundo componente principal y el tercer componente principal, entonces tal vez yo termino de De ejecutar de desarrollar mi modelo y lo llevo a la práctica y mi modelo predice bien, pero va a predecir bien en función de los componentes principales y de las de las relaciones que halló ese entre esos componentes principales, pero yo no voy a saber qué significa. Y ahí pierdo interpretabilidad que es importante en minería de datos.
01:26:01 Miguel Carlos Pita: Tener interpretabilidad de por qué se están produciendo determinadas situaciones está bueno que ustedes sepan a predijo bien. Mi algoritmo probablemente sea porque el libro era escrito portal autor y no por otro. Y con pescades ese análisis no lo voy a poder hacer lo voy a perder. Yo voy a saber que mi modelo predice bien, pero no voy a saber el factor de que está detrás que produce que Se esté ejecutando o desarrollando bien esa predicción importante tenerlo en cuenta.
01:26:34 Miguel Carlos Pita: Bueno, cómo hacer distintas técnicas de selección de atributos? Una que a veces se usa se usa mucho es la fuerza bruta yo. Si estoy apurado lo que hago es bueno agarro los que a mí entender o a mi criterio. Son importantes y me quedo con eso, si el resto lo saco eso puede llevar a que obviamente esté eliminando atributos que sean significativos o que sean importantes, eso se llama prueba y error. Lo que hago es probar.
01:27:13 Miguel Carlos Pita: Si esa selección de atributos es mejor que otra selección de atributos que pueda llegar a ser diferente al anterior. la prueba y la comparación estamos hablando de comparar la precisión del modelo, el resultado final que Todo esto que yo les estoy hablando de precisión del modelo de tasa de error, vamos a ver todos esos conceptos y vamos a vamos a dar a calcularlos y vamos a analizarlos.
01:27:37 Miguel Carlos Pita: Entonces yo comparo dos modelos distintos y me fijo, cual tiene mejor precisión que el otro o menor error que el otro. Otro este otra selección de atributos importante otra técnica de selección de atributos es el Miss ratio. Lo que yo hago es sacar del análisis, las columnas que tienen muchos datos faltantes. Entendiendo que probablemente no me entreguen información relevante o la poca información que posean no es significativa.
01:28:10 Miguel Carlos Pita: Otra técnica que podemos aplicar es el filtro de baja varianza o lo variance filter. Y acá vamos a empezar a hablar de estos conceptos que tienen relación con estadística, vamos a ver cómo hace cómo seguramente ustedes ya lo vieron en estadística, pero lo vamos a ver en Python cómo calcular una varianza como interpretar una varianza. Entonces si yo tengo poca varianza. Yo lo que puedo hacer es borrar ese atributo, sí, porque lo que me está indicando es que no tiene información, no tiene información relevante es atributo, por eso las cuestiones estadísticas todos los algoritmos que empezamos a aplicar ahora.
01:28:53 Miguel Carlos Pita: Trabajan con estadística y probabilidad entonces el filtro baja varianza analiza los atributos y aquellos que tienen es que tienen baja varianza que en este caso sería una varianza cercana a cero, nos estarían indicando de que la podemos borrar porque no están no están. No tienen información relevante intrínseca en Sí para aportar a mi modelo, entonces deben ser eliminados o pueden ser eliminados? Por eso mismo tenemos la inversa del filtro de baja varianza que es el filtro de alta correlación.
01:29:30 Miguel Carlos Pita: El filtro de alta correlación lo que hace es compara la correlación en los está hablando de un vínculo una dependencia lineal. Todo esto que yo les estoy mencionando ahora lo vamos a ver en futuras clases, pero la correlación nos habla de una dependencia lineal entre dos o más atributos. Si hay dos atributos que tienen una dependencia lineal, muy alta que significa eso que si un atributo sube, por ejemplo, yo estoy hablando. Estoy viendo la correlación entre el peso. Y perdón vamos entre la edad y la altura de una persona.
01:30:10 Miguel Carlos Pita: Entonces entre en pacientes de un año a 40 años entonces a medida que aumenta la edad a medio aumenta la altura les estoy dando un ejemplo burdo. Sí, obviamente los algoritmos se encuentran con relaciones entre datos entre atributos que nosotros ni nos podemos imaginar, pero para que se entienda el ejemplo yo quiero analizar la correlación entre. La altura y la edad de de personas en el rango etario de uno a 30 años de 140 años entonces la correlación a medida que aumenta la edad aumenta la altura sí, entonces tiene una una alta correlación esos patrones entonces generalmente para reducir la dimensionalidad para hacer una selección de atributos.
01:31:01 Miguel Carlos Pita: Yo si observo alta correlación entre atributos, puedo eliminar un atributo, por ejemplo digo bueno, si sube uno y sube el otro evidentemente está alta correlación lo que a mí me va a servir es trabajar solamente con un solo de estos atributos, sino con los dos, porque los dos me están entregando la misma información a mi modelo. Sí, yo lo que necesito es reducir la dimensionalidad y trabajar con un solo atributo de estos dos que tienen alta correlación entre sí está bien, ya vamos a establecer alguna diferencia a futuro, se los menciona simplemente, pero a futuro vamos a establecer una diferencia entre este tema de la alta correlación.
01:31:41 Miguel Carlos Pita: La alta correlación tiene que ser entre atributos llamados atributos característicos? Pero entre los atributos que son características de mi conjunto de datos que ya vamos a ver cuáles son necesito que la correlación sea si la correlación es alta, puedo trabajar con uno solo de ellos y elimino el otro. Eso ya lo vamos a hacer, pero si tengo una alta correlación entre un atributo característica y el atributo clase.
01:32:09 Miguel Carlos Pita: Que es el atributo que yo quiero predecir o la etiqueta de mi conjunto de datos, eso cuando veamos un conjunto de datos un data set les voy a mostrar. Bueno estos son los atributos característicos, este es el atributo clase para que lo tengan bien presente. Bueno, si yo tengo una alta correlación entre mi atributo entre un atributo característica y el atributo clase yo es atributo característica, lo tengo que mantener no lo puedo eliminar porque le aporte información a mi conjunto de datos.
01:32:37 Miguel Carlos Pita: Pero si tengo dos atributos del tipo característica con alta correlación, puedo eliminar uno de ellos y mantener el otro esto que les estoy mencionando es para que no tengan en cuenta, lo vamos a aplicar a futuro. Les dejo un enlace en donde todo esto está explicado más o menos yo me basa también para poder hablar sobre estas cuestiones, lo tienen ahí un poco más en detalle. tema de reducción de la dimensionalidad algunos algoritmos que ustedes han utilizado el random Forex o ensembel 3 esto ya los han utilizado, vamos a ver en esta materia las diferencias entre ambos entre ambos algoritmos y las distintas atributos y características que tienen Están muy buenos los vamos a graficar obviamente, vamos a graficar el rango Forex vamos a ver sus distintos sus distintas ramas, uno de raíz las relaciones que se establecen como como el random forest va ejecutando esa clasificación en mi conjunto de datos para poder predecir y obviamente eso esa clasificación que hace el random Forex lo que me termina llevando el incendio el 3.
01:33:55 Miguel Carlos Pita: Es un algoritmo muy similar al random, Forex ya vamos a ver su diferencia. Lo que me permite hacer es una reducción de la dimensionalidad. Sí, esto es algoritmos. Lo que hace obviamente es combina distintas posibilidades, combina varios árboles y elige el mejor el random forest elige el menor camino. Para que la precisión para que la predicción de mi algoritmo sea lo más efectiva posible. Entonces elige el mejor camino creo que ustedes han hecho random Forex pero creo que lo han algunos lo han mencionado en una de las primeras clases, así que seguro que lo han visto.
01:34:33 Miguel Carlos Pita: Igual lo vamos a volver a ver, pero lo que vemos, lo vemos aplicado esta materia. Sí, así que lo que hayan visto en otras lo que les va a permitir a ustedes es reforzar lo que ya saben y aparte ver nuevos conceptos y nuevas aplicaciones, entonces es todo positivo todo suma el conocimiento que tienen que tener. Bueno, otras técnicas es el forward future construction, va a jugar elimination? Un poco de esto hablé ayer.
01:35:05 Miguel Carlos Pita: Esto viene un poco a relación también con lo que es la prueba y el error que vimos anteriormente en la placa anterior, cómo funcionan estos dos métodos. Entonces con el software construction, yo tengo un conjunto de datos. Al cual le agrego un atributo una columna sí, una columna es un atributo de mi conjunto de datos, entonces yo agrego un atributo. mi modelo predice obtengo un resultado Luego agregó una columna más mi algoritmo predice obtengo un resultado si el desempeño o el rendimiento fue peor al anterior significa que ese atributo que agregue.
01:35:48 Miguel Carlos Pita: No es conveniente para el modelo que estoy aplicando para el modelo que estoy ejecutando. Entonces voy atrás y agrego, otro atributo diferente. Sí, si el desempeño mejora mantengo esos dos atributos y así sucesivamente voy haciendo esa prueba. Yo es lo que les menciono así hay algoritmos que esto lo hacen automáticamente. Está bien, pero cuando hablemos bueno, ahora vamos a hacer, vamos a aplicar forward future construction. Y nosotros vamos a hacer un par de líneas de código por detrás el algoritmo está ejecutando este proceso se entiende y ustedes tienen que entender lo que están haciendo.
01:36:25 Miguel Carlos Pita: Porque si no para aprendernos líneas de código de memoria, a eso lo hace cualquiera y tampoco los que lo hace cualquiera, pero si ustedes no entienden el proceso que están aplicando. Les puedo asegurar que al tercer paso del del modelado se van a perder y no van a entender que están haciendo y para qué entonces cuando empecemos a ver todos estos conceptos vamos a ir viendo sus diferencias cuando se aplican qué es lo que hace el algoritmo en sí, porque muchas veces cuando apliquemos un modelo.
01:36:52 Miguel Carlos Pita: Y el proceso y generalmente aplicando la librería cyd learn, que les mencioné al principio de la materia el proceso está detrás lo ejecuta la librería. Nosotros en general la carga computacional más fuerte no la hacemos, sí, nosotros aplicamos líneas de código lo que tenemos que saber es que se ejecuta por detrás. Para poder tener criterio a la hora de tomar una decisión de qué modelo de qué técnica utilizada ahí está? La clave de esta materia bueno y el backwards future elimination es el mismo concepto pero a la inversa yo en vez de empezar con una columna y agrego otra y agregó otra empiezo con todas las columnas.
01:37:34 Miguel Carlos Pita: Predigo obtengo un resultado luego quito una columna predigo obtengo un resultado, si la predicción fue mejor significa que esa columna que saque evidentemente no me estaba aportando información o era contraproducente para el desempeño de mi modelo. Y continúo con ese proceso de manera iterativa. Nuevamente este algoritmo se ejecuta de manera automática también. Pero hay que saber que lo que se ejecuta cuando uno lo aplica. Bueno y técnicas de revolución de la dimensionada de la dimensionalidad, perdón, como principal componente análisis como principal componente análisis o singular varios deviation que son técnicas de álgebra lineal como ya les mencioné.
01:38:23 Miguel Carlos Pita: Se crea un nuevo espacio dimensional con menos dimensiones yo arranco con muchas dimensiones. Se transforma ese espacio y dimensiones en a través de combinaciones lineales. En un espacio dimensional menor y obviamente perdemos interpretabilidad que es lo que yo les mencionaba anteriormente, sí. A ver qué más dice ahí aparece algo que importante mencionar. Está bueno es el último renglón los últimos dos renglones es la clave. Sí, si pierdo interpretabilidad eso tiene un valor en minería de datos, no sólo queremos clasificar bien o predecir bien, sino saber por qué? Porque yo lo que quiero obtener es conocimiento. Está bien excelente que mi algoritmo prediga bien, la verdad que está bárbaro. Pero si puedo saber por qué lo hace mejor. Bueno, este esto lo saqué.
01:39:24 Miguel Carlos Pita: De el de la página que es que está el enlace ahí colocado. Básicamente no nos muestra distintas estimaciones de precisión. Por ejemplo no no son distintas técnicas. Y me dice cuál es el porcentaje de reducción del algoritmo del conjunto de datos, cuál fue la precisión en el conjunto de datos de prueba o de validación? Cuál fue la precisión que se obtuvo esto esto desconozco sinceramente les miento si si les digo que qué significa abuse es área bajo la curva área under the core. Y es una es una técnica que también vamos a ver que lo que nos está indicando es cuán bien clasifica nuestro modelo. Sí, ya vamos a ver cómo interpretar un área bajo la curva como aplicarla y cómo interpretarla sí, lo que nos dice es una medida de estimación de cuán bien clasificó un modelo entonces en definitiva me compara varias.
01:40:33 Miguel Carlos Pita: Varias técnicas y bueno la conclusión que se obtiene en esta página como les digo ustedes después pueden leerlo y pueden observarla. Es que en un ejemplo aplicado en ella el algoritmo que más que mejor corresponde y que mejor que respondería aplicar sería el de Random Forrest o ensembel 3. Sí, ya que tiene tal vez no tenga la mejor tasa de reducción fíjense que tiene un 86% en comparación con los backwards elimination o forward construction que tienen mayor tasa de reducción de mi conjunto de datos en el concepto de selección de atributos o reducción de dimensionalidad. Tal vez no tengan también la mayor la mayor tasa de acierto.
01:41:15 Miguel Carlos Pita: Sí, porque tienen un 76% y estos tienen más, pero si tiene también una mayor área bajo la curva, o sea, este este concepto es mayor que los otros y en y si analizo los tres. Las tres columnas es el que mejor desempeño tiene sí, entonces es simplemente a modo ilustrativo y nosotros vamos a hacer este trabajo. Vamos a hacer este trabajo, vamos a comparar distintas técnicas, vamos a perdón, vamos a compararlas entre sí y vamos a seleccionar la mejor de ellas.
01:41:49 Miguel Carlos Pita: Está bien, vamos a hacer todo ese análisis y todo ese proceso lo vamos a ejecutar, lo vamos a ejecutar manualmente y después vamos a ver algunos algoritmos ya sobre el final de la materia. Que esto lo hacen de manera automática. Sí, entonces uno no aprende a sumar y después con la calculadora, le pongo un ejemplo muy burdo, porque está buenísimo aprender el algoritmo que me hace todo, pero acá tenemos que aprender a modelar bien y el modelado correctamente se hace sabiendo cada técnica que significa y cómo se aplica si nosotros vamos de una al algoritmo automático que nos hace todo por por sí solo.
01:42:33 Miguel Carlos Pita: Y nuestro trabajo este pierde sentido porque en muchas ocasiones el algoritmo deja de tener en cuenta algunas cuestiones que nosotros a través de la experiencia del conocimiento que tenemos podemos aplicar en nuestro modelados, porque el algoritmo tiene sus limitaciones también no se crean que es perfecto el mejor modelado lo van a hacer ustedes. Aplicando bien las distintas estrategias que vayamos viendo a lo largo de la materia.
01:43:00 Miguel Carlos Pita: Y luego de eso como una manera de seleccionar algún alguna técnica en particular pueden utilizar el algoritmo pero el algoritmo tampoco le soluciona todo el modelado la solución es solamente una etapa una parte. bueno, vamos a ir ya, ya estamos terminando PSA Ya mencioné atento a esto se utiliza para atributos continuos sí, atributos de tipo numéricos. Ojo con eso si yo no pese a no se aplica para atributos categóricos.
01:43:40 Miguel Carlos Pita: Lo que se busca como mencionaba son combinaciones lineales y esas combinaciones lineales. Lo que hacen es tratar de obtener o capturar la máxima variación de los datos para poder establecer esas combinaciones. Otra técnica para hacer una selección de atributos es como les mencionaba dar peso a los atributos. Hay atributos que son más importantes que otros a la hora de ejecutar nuestro modelo. O que tienen más relevancia que otros vamos a ver soporte machine.
01:44:14 Miguel Carlos Pita: Que el peso los tiene automáticamente, pero también vamos a ver técnicas, por ejemplo como la normalización que además de permitirme dar peso a los atributos y de llevar. la escala de los atributos a la misma perdón, todos los atributos a la misma escala darles a todos al mismo peso y que no haya uno que tenga más relevancia que otros darles a todos al mismo tiempo el peso para que todos sean igual de importantes a la hora de que me dé algoritmo o que de mi modelo haga sus predicciones también vamos a ver que por ejemplo, el random forest también tiene la posibilidad de Conocer el peso de los atributos cuando nosotros hacemos un rato Forex y lo graficamos el camino el mejor camino de clasificación que obtiene el random forest lo hace en función de los pesos de los atributos, o sea que también vamos a ver que hay técnicas de reducción de dimensionalidad que me permiten ejecutar también el peso o conocer el peso de cada uno de los atributos.
01:45:18 Miguel Carlos Pita: Otra técnica que vamos a trabajar para hacer pre procesamiento de datos es la creación de atributos tiene mucha relación con lo que habíamos visto al principio de la clase acerca de la agregación, sí de atributos. de la combinación de atributos y de obtener un nuevo atributo bueno Tal vez ustedes hablen o observen en alguna bibliografía o cuando estén viendo crear atributos y puede tener. semejanzas con el concepto de la agregación de atributos Cómo se crean atributos? Por ejemplo es en algún caso mapeando atributos a un nuevo espacio ya vamos a ver este concepto ahora la próxima diapositiva que significa mapear atributos.
01:46:06 Miguel Carlos Pita: un ejemplo Un ejemplo es lo que veíamos en PSA el PCA que hace una una combinación lineal y transforma. 20 dimensiones que tengo en tres dimensiones así mediante una combinación lineal Está creando nuevos atributos esos nuevos atributos a partir de ahora son las combinaciones lineales de las de los 20 anteriores atributos. Empecé a yo puedo elegir si quiero tres componentes principales cinco componentes principales. Pero esos tres componentes principales son nuevos atributos creados a través de un mapeo de atributos a un nuevo espacio ese nuevo espacio es un nuevo espacio de combinaciones lineales de todos los otros atributos. Ya vamos a hablar de PCA más adelante y lo vamos a poner en práctica.
01:47:00 Miguel Carlos Pita: O construir atributos, sí, el mismo ejemplo que mencionaba en la agregación de atributos, entonces cuando hablemos de creación de atributos o de agregación son conceptos similares, sí, el mismo ejemplo estoy hablando de la valoración positiva en base unos tuits, si los tweets tienen valoración positiva, pueden ser debido a una festividad o si tienen valoración negativa debido a algún suceso que lo produzca sí. Entonces construye un nuevo atributo en función de los tuits que son obviamente.
01:47:32 Miguel Carlos Pita: Datos para nosotros y esos datos tienen una valoración positiva o negativa entonces para volver a mencionarlo del mapeo que hable del mapeo con relación a lo que estaba comentando anteriormente de PSA un simple ejemplo. Yo que soy electrónico les puedo mencionar. un un mapeo que o tal vez ustedes alguna vez lo hayan escuchado esto se conoce como transformada de Fourier Pero cuál es el concepto de la transformada de Fourier yo trabajo con una con dos ondas? Dos ondas sinusoidales sí, sí tengo dos ondas, si no soy Gales y este primer gráfico está en el espectro o en el en el en el espacio del tiempo. Sí, yo lo estoy mapeando en el tiempo. Entonces tengo una onda que transcurre a través del tiempo con una frecuencia determinada.
01:48:33 Miguel Carlos Pita: Pero puede suceder que yo estas ondas tengan ruidos, sí, entonces yo deje de percibir producto de este ruido. Que también en en data Science los datos tienen ruido. Yo lo que puedo percibe no puedo percibir esas ondas, entonces si yo estoy observando las ondas en el espectro del tiempo. No puedo apreciar su información entonces lo que hay que hacer es mapear. Es mapear a un nuevo espacio en lo que es análisis matemático se utiliza la transformada de Fourier y lo que me permite transformar de Fourier es hacer una transformación un mapeo desde el espacio temporal al espacio de la frecuencia.
01:49:24 Miguel Carlos Pita: Sí, entonces se acaba es como que se cambia la perspectiva desde la cual estoy observando las ondas entonces a través del mapeo de este espacio temporal al espacio de la frecuencia. Lo que puedo observar es por ejemplo, obtener información distinta de la que ellos tenían el espacio temporal. Entonces aquí voy a voy a ver estos picos o están representando, cuál es la frecuencia de cada una de estas ondas que yo en el en segunda instancia, no podía observar debido al ruido, que estaba que estaba incorporado en en las señales. Aquí hago referencia con esto que cuando ustedes mapean un nuevo espacio.
01:50:07 Miguel Carlos Pita: Analizan los los datos desde otra perspectiva con PCA con principal componente análisis están mapeando a un nuevo espacio. Ustedes de esa manera pierden interpretabilidad ya no ya dejan de interpretar el espacio ya no saben porque eso ese nuevo espacio es generado mediante combinaciones lineales de sus atributos principales, pero a través de esa combinación lineal ustedes reducen. Reducen la dimensionalidad de su conjunto de datos tienen menos. costo computacional mayor tiempo de procesamiento mayor tiempo, perdón Me rectifico menor tiempo de procesamiento y menor tiempo de aprendizaje de su modelo y va a ejecutar mejores predicciones, porque los componentes principales son combinaciones lineales de todos los atributos, su modelo va a predecir más rápido y lo va a hacer de manera más eficiente.
01:51:08 Miguel Carlos Pita: Porque trabaja con las relaciones entre los atributos las relaciones que ya encontró ese algoritmo de PSA y eso lo hizo a través del mapeo nuevo espacio. discretizar El tema del discretizado es el hecho de decidir cómo dividir o subdividir a través de categorías? El conjunto de datos y eso al algoritmos que lo ejecutan de manera automática, pero bueno mediante un ejemplo. Imagínense que yo tengo un conjunto de datos y quiero dividir valores como alto medio bajo. Bueno, ese es un ejemplo pero vamos a verlo con un gráfico.
01:51:52 Miguel Carlos Pita: Yo puedo tener distintas estrategias, sí, yo tengo mi conjunto de datos. Están representando. En este caso, por ejemplo, el ingreso de las personas, pero puede ser cualquier cosa que ustedes se les ocurra lo que estamos viendo acá es cómo hacer una discretización y cómo se aplica yo puedo discretizar o sea categorizar mi conjunto de datos estableciendo intervalos iguales.
01:52:10 Emilce Alconz: Hola, estoy transcribiendo esta llamada con mi extensión Tactiq AI: https://tactiq.io/r/transcribing
01:52:15 Miguel Carlos Pita: Intervalos de igual ancho es decir si acá tengo 5 10 15 y 20 de 5 en 5. Pero evidentemente. Tal vez no esté discretizando correctamente fíjense los las nubes de puntos de colores, están representando datos que tienen. Lo que se llamarían clusters en minería de datos que tienen propiedades similares, sí, obviamente mediante esta discretización estoy haciendo una mala una mala. una mala de una mala discretización la redundancia Si lo hago igual frecuencia, igual frecuencia quiere decir si yo quiero dividir la misma cantidad de puntos, sí.
01:52:58 Miguel Carlos Pita: Si yo quiero que el primer segmento tenga la misma cantidad de puntos que el segundo segmento la misma cantidad de puntos que el tercer segmento o que el cuarto segmento evidentemente tampoco voy a discretizar de forma correcta entonces para poder hacer esta discretización de manera adecuada. Uno de los algoritmos que nos permite dividir nuestro conjunto de datos o discretizarlo en clusters a través de propiedades similares que tal vez para nosotros sean irreconocibles.
01:53:26 Miguel Carlos Pita: Pero los algoritmos permiten hallar los patrones subyacientes subyacentes detrás de los datos uno de ellos es el algoritmo camins, lo vamos a ver lo vamos a trabajar y vamos a ver otros también. Y por último ya con esto terminamos otra otra manera de pre procesar los datos es a través de la transformación de atributos, si yo por ejemplo un atributo le aplica una potencia x elevado a la k. Si ya esto lo vamos a ver para la próxima clase.
01:53:58 Miguel Carlos Pita: Empezaremos viendo estas cuestiones, cómo hacer una potencia un logaritmo después seguimos avanzando, pero estos transformaron datos, sí, yo el atributo que un atributo este resultante de ejecutar esta operación entre dos atributos es una transformación de atributos, lo mismo al aplicar una función logarítmica, lo mismo aplicar las funciones exponencial del valor de Euler e elevado a un atributo determinado lo mismo aplicar el valor absoluto o el módulo.
01:54:31 Miguel Carlos Pita: De un atributo este son transformaciones simples sí, obviamente estamos hablando en este caso de funciones matemáticas u operaciones aritméticas. Vamos a ver, estas vamos a ver un montón de funciones. Y bueno ni hablar que también transformar un atributo es algunas de las cosas que ya hablamos antes como estandarizar o como normalizar y acá hay una diferencia como lo yo les decía anteriormente en algunos casos. Alguna bibliografía habla de normalización? Y y en otra se habla de estandarización como la misma cosa. Yo les voy a decir lo que a mí entender en función de lo que yo.
01:55:17 Miguel Carlos Pita: Lo vamos a aplicar de manera aritmética y lo vamos a aplicar a través de un
01:55:40 Miguel Carlos Pita: algoritmo y vamos a ver cómo se comportan ambas transformaciones y qué es lo que estamos tratando de obtener cuando las aplicamos son importantes estas transformaciones porque es una etapa fundamental para luego aplicar otros algoritmos, por ejemplo, PSA trabaja mejor si tengo un algoritmo, sí, perdón, si tengo atributos normalizados. Todas estas cuestiones que venimos hablando en futuras clases cuando veamos en detalle esos algoritmos esas recomendaciones están escritas y se las hago para que tengan y para que sepan que ustedes pueden aplicar PCA por estoy hablando de ese algoritmo particularmente sin hacer una transformación de normalización.
01:56:23 Miguel Carlos Pita: La van a aplicar, pero el algoritmo funciona mejor porque estadísticamente está preparado para que los algoritmos con los que trabajo para que los atributos con los que trabaje sean atributos normalizados de esa manera, los componentes principales van a representar fielmente las combinaciones lineales de los distintos atributos en cambio, si yo no normalizo antes eso pueden ocurrir. Entonces esos detalles hay que tenerlos en cuenta y los iremos charlando.
01:56:52 Miguel Carlos Pita: Las clases siguientes bueno, espero no, no haberlos aburrido demasiado esto ya es lo último. de la presentación una cosa es darle la presentación y y que lo ustedes lo vean por su cuenta pero Pero si no lo charlamos a esto, hay conceptos importantes que no se mencionan y creo que tenemos mucha tela para cortar y lo importante es es que lo que todas las clases avancemos un poquito más. Sí en conocer estos estos conceptos y a partir de la clase que viene vamos a empezar ya con con Júpiter sí vamos a empezar.
01:57:31 Miguel Carlos Pita: El martes que viene con conceptos empezando a aprendiendo a utilizar lo que es Júpiter notebook y después empezaremos con Python sí vamos a ir la casa que viene empezaremos desde lo más básico, pero no vamos a ir.
  